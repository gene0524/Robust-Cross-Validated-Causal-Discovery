{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Agent-Based Model Validation (TODO)\n","The five-step validation process for agent-based models as described by Guerini and Moneta (2017). The steps include:\n","1. Dataset uniformity\n","2. Analysis of ABM properties\n","3. VAR estimation and identification\n","4. Causal structure identification\n","5. Validation assessment"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from statsmodels.tsa.api import VAR\n","from scipy import stats, interpolate\n","from sklearn.decomposition import FastICA\n","import networkx as nx\n","from itertools import combinations, permutations\n","# from lingam import VARLiNGAM      # The original version of VARLiNGAM\n","from lingam2 import VARLiNGAM       # The modified version of VARLiNGAM\n","# from statsmodels.tsa.stattools import adfuller        # For the Augmented Dickey-Fuller test to ensure stationarity"]},{"cell_type":"markdown","metadata":{},"source":["## Step 1: Dataset Uniformity\n","We begin by selecting the variables of interest and preparing the datasets from real-world data (RW-data) and agent-based model simulations (AB-data)."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def uniform_datasets(RW_data, AB_data, columns, log_vars=None, pct_vars=None):\n","    if log_vars is None:\n","        log_vars = []\n","    if pct_vars is None:\n","        pct_vars = []\n","\n","    # Convert column names to indices\n","    log_indices = [columns.index(var) for var in log_vars if var in columns]\n","    pct_indices = [columns.index(var) for var in pct_vars if var in columns]\n","\n","    T_RW = RW_data.shape[0]\n","    T_AB = AB_data.shape[1]\n","\n","    # Ensure that the datasets have the same time dimension\n","    if T_AB > T_RW:\n","        AB_data = AB_data[:, -T_RW:, :]\n","    else:\n","        RW_data = RW_data[-T_AB:, :]\n","\n","    # Apply log transformation\n","    for idx in log_indices:\n","        RW_data[:, idx] = np.log(RW_data[:, idx])\n","        AB_data[:, :, idx] = np.log(AB_data[:, :, idx])\n","    \n","    # Convert to percentages\n","    for idx in pct_indices:\n","        if RW_data[:, idx].mean() > 1:\n","            RW_data[:, idx] *= 100\n","        if AB_data[:, :, idx].mean() > 1:\n","            AB_data[:, :, idx] *= 100\n","    \n","    return RW_data, AB_data"]},{"cell_type":"markdown","metadata":{},"source":["## Step 2: Analysis of ABM Properties\n","We check for statistical equilibrium and ergodicity of the ABM properties."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Step 2: Analysis of ABM properties\n","\n","# Test for statistical equilibrium across different time periods in the ABM data.\n","def test_statistical_equilibrium(AB_data):\n","    M, T, K = AB_data.shape\n","    p_values = np.zeros((K, T*(T-1)//2))\n","    \n","    for k in range(K):\n","        idx = 0\n","        for i in range(T):\n","            for j in range(i+1, T):\n","                _, p = stats.ks_2samp(AB_data[:, i, k], AB_data[:, j, k])\n","                p_values[k, idx] = p\n","                idx += 1\n","    \n","    # return the proportion of p-values that are greater than 0.05\n","    return np.mean(p_values > 0.05, axis=1)\n","\n","# Test for ergodicity in the ABM data.\n","def test_ergodicity(AB_data):\n","    M, T, K = AB_data.shape\n","    p_values = np.zeros((K, M*T))\n","    \n","    for k in range(K):\n","        idx = 0\n","        for i in range(M):\n","            for j in range(T):\n","                _, p = stats.ks_2samp(AB_data[i, :, k], AB_data[:, j, k])\n","                p_values[k, idx] = p\n","                idx += 1\n","\n","    # return the proportion of p-values that are greater than 0.05\n","    return np.mean(p_values > 0.05, axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["## Step 3: VAR Estimation and Identification\n","We estimate the reduced-form VAR model for both RW-data and AB-data."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def handle_inf_nan(data, method='interpolate'):\n","    problem_mask = np.isnan(data) | np.isinf(data)\n","    \n","    if problem_mask.any():\n","        problem_rows = np.where(problem_mask.any(axis=1))[0]\n","        # print(f\"Warning: Data contains inf or nan values at rows: {problem_rows}\")\n","        \n","        if method == 'interpolate':\n","            for col in range(data.shape[1]):\n","                mask = problem_mask[:, col]\n","                if mask.any():\n","                    good = ~mask\n","                    f = interpolate.interp1d(np.flatnonzero(good), data[good, col], \n","                                             bounds_error=False, fill_value='extrapolate')\n","                    data[mask, col] = f(np.flatnonzero(mask))\n","        elif method in ['ffill', 'bfill']:\n","            data = pd.DataFrame(data).fillna(method=method).values\n","        elif method == 'mean':\n","            col_means = np.nanmean(data, axis=0)\n","            data[problem_mask] = np.take(col_means, np.where(problem_mask)[1])\n","        else:\n","            raise ValueError(\"Invalid method. Choose 'interpolate', 'ffill', 'bfill', or 'mean'.\")\n","        \n","        print(f\"Handled {np.sum(problem_mask)} inf/nan values using {method} method.\")\n","    \n","    return data\n","\n","def estimate_var(data, max_lags=8, inf_nan_method='interpolate'):\n","    data = handle_inf_nan(data, method=inf_nan_method)\n","    \n","    try:\n","        model = VAR(data)\n","        results = model.fit(maxlags=max_lags, ic='bic')\n","        return results\n","    except ValueError as e:\n","        print(f\"Error fitting VAR model: {e}\")\n","        return None"]},{"cell_type":"markdown","metadata":{},"source":["## Step 4: Causal Structure Identification\n","We identify the causal structure using PC-algorithm and VAR-LiNGAM."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Step 4: SVAR identification\n","def partial_corr(C):\n","    C = np.asarray(C)\n","    p = C.shape[1]\n","    P_corr = np.zeros((p, p), dtype=float)\n","    for i in range(p):\n","        P_corr[i, i] = 1\n","        for j in range(i+1, p):\n","            idx = np.ones(p, dtype=bool)\n","            idx[i] = False\n","            idx[j] = False\n","            beta_i = np.linalg.lstsq(C[:, idx], C[:, j], rcond=None)[0]\n","            beta_j = np.linalg.lstsq(C[:, idx], C[:, i], rcond=None)[0]\n","            res_j = C[:, j] - C[:, idx].dot(beta_i)\n","            res_i = C[:, i] - C[:, idx].dot(beta_j)\n","            corr = stats.pearsonr(res_i, res_j)[0]\n","            P_corr[i, j] = corr\n","            P_corr[j, i] = corr\n","    return P_corr\n","\n","def pc_algorithm(data, alpha=0.05):\n","    n, p = data.shape\n","    G = nx.Graph()\n","    G.add_nodes_from(range(p))\n","    G.add_edges_from(combinations(range(p), 2))\n","    \n","    #  Test for conditional independence\n","    def conditional_independence(X, Y, Z):\n","        if not Z:\n","            corr, p_value = stats.pearsonr(data[:, X], data[:, Y]) # Pearson correlation\n","            return p_value > alpha\n","        else:\n","            pcorr = partial_corr(data[:, [X, Y] + list(Z)])\n","            t_stat = pcorr[0, 1] * np.sqrt((n - len(Z) - 2) / (1 - pcorr[0, 1]**2))\n","            p_value = 2 * (1 - stats.t.cdf(abs(t_stat), n - len(Z) - 2))\n","            return p_value > alpha\n","    \n","    l = 0\n","    while any(len(list(G.neighbors(node))) >= l for node in G.nodes()):\n","        for (X, Y) in permutations(G.nodes(), 2):\n","            if G.has_edge(X, Y):\n","                Z = list(set(G.neighbors(X)) - {Y})\n","                if len(Z) >= l:\n","                    for S in combinations(Z, l):\n","                        if conditional_independence(X, Y, S):\n","                            G.remove_edge(X, Y)\n","                            break\n","        l += 1\n","    \n","    dag = nx.DiGraph()\n","    dag.add_nodes_from(G.nodes())\n","    for (X, Y) in G.edges():\n","        dag.add_edge(X, Y)\n","    \n","    return nx.adjacency_matrix(dag).todense()\n","\n","def var_lingam(residuals):\n","    ica = FastICA(n_components=residuals.shape[1], random_state=0)\n","    S = ica.fit_transform(residuals)\n","    W = ica.mixing_\n","    \n","    P = np.zeros_like(W)\n","    remaining = list(range(W.shape[0]))\n","    for i in range(W.shape[0]):\n","        j = remaining[np.argmin([np.sum(np.abs(W[j, remaining])) for j in remaining])]\n","        P[i, j] = 1\n","        remaining.remove(j)\n","    \n","    W_perm = P.dot(W)\n","    B = np.eye(W.shape[0]) - np.linalg.inv(W_perm)\n","    B[np.triu_indices(B.shape[0], 1)] = 0\n","    \n","    return B, P\n","\n","def is_gaussian_multi_test(residuals, significance_level=0.05):\n","    n_vars = residuals.shape[1]\n","    \n","    shapiro_p_values = [stats.shapiro(residuals[:, i])[1] for i in range(n_vars)]\n","    anderson_results = [stats.anderson(residuals[:, i], 'norm') for i in range(n_vars)]\n","    \n","    shapiro_gaussian = sum(p > significance_level for p in shapiro_p_values) > n_vars / 2\n","    anderson_gaussian = sum(result.statistic < result.critical_values[2] for result in anderson_results) > n_vars / 2\n","    \n","    print(f\"Shapiro-Wilk test results: {shapiro_gaussian}\")\n","    print(f\"Anderson-Darling test results: {anderson_gaussian}\")\n","    \n","    return shapiro_gaussian and anderson_gaussian\n","\n","def svar_identification(residuals, var_results):\n","    is_gaussian = is_gaussian_multi_test(residuals)\n","    \n","    if is_gaussian:\n","        print(\"Using PC algorithm (Gaussian residuals)\")\n","        B = pc_algorithm(residuals)\n","    else:\n","        print(\"Using VAR-LiNGAM algorithm (non-Gaussian residuals)\")\n","        model = VARLiNGAM()\n","        result = model.fit(residuals)\n","        # result = model.bootstrap(residuals, n_sampling=100)\n","        print(result.adjacency_matrices_)\n","        B = result.adjacency_matrices_[0]\n","        # B, _ = var_lingam(residuals)\n","    \n","    # Gamma0 = np.linalg.inv(np.eye(B.shape[0]) - B)\n","    # Gamma = [Gamma0 @ coef for coef in var_results.coefs]\n","    \n","    return # Gamma0, Gamma"]},{"cell_type":"markdown","metadata":{},"source":["## Step 5: Validation Assessment\n","We compare the estimated causal structures using sign, size, and conjugate similarity tests."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Step 5: Validation assessment\n","def compute_similarity(RW_structure, AB_structure):\n","    sign_similarity = np.mean(np.sign(RW_structure) == np.sign(AB_structure))\n","    \n","    size_similarity = np.mean(\n","        (AB_structure >= RW_structure - 2*np.std(RW_structure)) & \n","        (AB_structure <= RW_structure + 2*np.std(RW_structure))\n","    )\n","    \n","    conj_similarity = np.mean(\n","        (np.sign(RW_structure) == np.sign(AB_structure)) & \n","        (AB_structure >= RW_structure - 2*np.std(RW_structure)) & \n","        (AB_structure <= RW_structure + 2*np.std(RW_structure))\n","    )\n","    \n","    return sign_similarity, size_similarity, conj_similarity\n","\n","def analyze_lagged_effects(RW_Gamma, AB_Gammas):\n","    num_lags = len(RW_Gamma)\n","    num_vars = RW_Gamma[0].shape[0]\n","    \n","    lag_similarities = np.zeros((num_lags, 3))  # For sign, size, and conjunction similarities\n","    \n","    for lag in range(num_lags):\n","        lag_similarities[lag, 0] = np.mean([compute_similarity(RW_Gamma[lag], AB_Gamma[lag])[0] for AB_Gamma in AB_Gammas])\n","        lag_similarities[lag, 1] = np.mean([compute_similarity(RW_Gamma[lag], AB_Gamma[lag])[1] for AB_Gamma in AB_Gammas])\n","        lag_similarities[lag, 2] = np.mean([compute_similarity(RW_Gamma[lag], AB_Gamma[lag])[2] for AB_Gamma in AB_Gammas])\n","    \n","    return lag_similarities"]},{"cell_type":"markdown","metadata":{},"source":["## Running the Code\n","We run the five-step validation process on the given ABM and RW-data."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["RW_data shape: (200, 6)\n","AB_data shape: (10, 200, 6)\n"]},{"name":"stderr","output_type":"stream","text":["/var/folders/c9/n713l58d4h7cvf9l3l77fx500000gn/T/ipykernel_15258/3372547065.py:23: RuntimeWarning: invalid value encountered in log\n","  AB_data[:, :, idx] = np.log(AB_data[:, :, idx])\n"]},{"name":"stdout","output_type":"stream","text":["\n","Statistical Equilibrium Results:\n","[0.32170854 0.98562814 0.98638191 0.99351759 0.9919598  0.9860804 ]\n","\n","Ergodicity Results:\n","[0.2255 0.9475 0.948  0.9615 0.9665 0.9595]\n","\n","Handled 1 inf/nan values using interpolate method.\n","Handled 1 inf/nan values using interpolate method.\n","Shapiro-Wilk test results: False\n","Anderson-Darling test results: False\n","Using VAR-LiNGAM algorithm (non-Gaussian residuals)\n","lag number:1\n","Btaus len:2\n","[[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n","   0.00000000e+00 0.00000000e+00]\n","  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n","   0.00000000e+00 0.00000000e+00]\n","  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n","   0.00000000e+00 0.00000000e+00]\n","  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n","   0.00000000e+00 0.00000000e+00]\n","  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n","   0.00000000e+00 0.00000000e+00]\n","  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n","   0.00000000e+00 0.00000000e+00]]\n","\n"," [[8.43972612e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n","   0.00000000e+00 0.00000000e+00]\n","  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n","   0.00000000e+00 0.00000000e+00]\n","  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n","   0.00000000e+00 0.00000000e+00]\n","  [0.00000000e+00 0.00000000e+00 9.83121275e-05 0.00000000e+00\n","   0.00000000e+00 0.00000000e+00]\n","  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n","   0.00000000e+00 0.00000000e+00]\n","  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n","   0.00000000e+00 0.00000000e+00]]]\n"]}],"source":["# Load real world data\n","RW_data = pd.read_csv('data/sample/real_world_data.csv')\n","columns = RW_data.columns.tolist()\n","RW_data = RW_data.values\n","\n","# Load ABM data\n","AB_df = pd.read_csv('data/sample/abm_simulated_data.csv')\n","AB_data = []\n","for i in range(10):  # Number of Monte Carlo simulations\n","    sim_data = AB_df[AB_df['simulation'] == i].drop(['simulation', 'timestep'], axis=1).values\n","    AB_data.append(sim_data)\n","AB_data = np.array(AB_data)\n","\n","print(\"RW_data shape:\", RW_data.shape)\n","print(\"AB_data shape:\", AB_data.shape)\n","\n","# Data preprocessing\n","log_vars = ['C', 'I', 'Y', 'P']     # Variables to log-transform\n","pct_vars = ['U', 'R']               # Variables to convert to percentages\n","\n","\n","# Step 1: Dataset uniformity\n","RW_data, AB_data = uniform_datasets(RW_data, AB_data, columns, log_vars=log_vars, pct_vars=pct_vars)\n","\n","# Step 2: Analysis of ABM properties\n","equilibrium_results = test_statistical_equilibrium(AB_data)\n","ergodicity_results = test_ergodicity(AB_data)\n","\n","print(\"\\nStatistical Equilibrium Results:\")\n","print(equilibrium_results)\n","print(\"\\nErgodicity Results:\")\n","print(ergodicity_results)\n","print()\n","\n","# Step 3: VAR Estimation\n","RW_var_results = estimate_var(RW_data)\n","AB_var_results = [estimate_var(AB_data[i]) for i in range(AB_data.shape[0])]\n","\n","# Step 4: SVAR Identification\n","svar_identification(RW_data, RW_var_results)\n","# RW_Gamma0, RW_Gamma = svar_identification(RW_var_results.resid, RW_var_results)\n","# AB_Gamma0s, AB_Gammas = zip(*[svar_identification(result.resid, result) for result in AB_var_results])\n","\n","# # Convert to numpy arrays for easier manipulation\n","# RW_Gamma0 = np.array(RW_Gamma0)\n","# RW_Gamma = np.array(RW_Gamma)\n","# AB_Gamma0s = np.array(AB_Gamma0s)\n","# AB_Gammas = np.array(AB_Gammas)\n","\n","# print(\"\\nRW_Gamma0 shape:\", RW_Gamma0.shape)\n","# print(\"RW_Gamma shape:\", RW_Gamma.shape)\n","# print(\"AB_Gamma0s shape:\", AB_Gamma0s.shape)\n","# print(\"AB_Gammas shape:\", AB_Gammas.shape)\n","\n","# # Step 5: Validation Assessment\n","# similarities = []\n","# for i in range(len(AB_Gamma0s)):\n","#     sign_sim, size_sim, conj_sim = compute_similarity(RW_Gamma0, AB_Gamma0s[i])\n","#     similarities.append((sign_sim, size_sim, conj_sim))\n","\n","# similarities = np.array(similarities)\n","\n","# # Compute average similarities across all Monte Carlo simulations\n","# avg_similarities = np.mean(similarities, axis=0)\n","# std_similarities = np.std(similarities, axis=0)\n","\n","# print(\"\\nAverage Similarities (Sign, Size, Conjunction):\")\n","# print(avg_similarities)\n","# print(\"\\nStandard Deviation of Similarities:\")\n","# print(std_similarities)\n","\n","# # Visualize results\n","# labels = ['Sign-based', 'Size-based', 'Conjunction']\n","# x = np.arange(len(labels))\n","# width = 0.35\n","\n","# fig, ax = plt.subplots(figsize=(10,6))\n","# rects1 = ax.bar(x - width/2, avg_similarities, width, label='Mean', yerr=std_similarities,\n","#                 align='center', alpha=0.8, ecolor='black', capsize=10)\n","\n","# ax.set_ylabel('Similarity')\n","# ax.set_title('Similarity Measures between RW and AB Model')\n","# ax.set_xticks(x)\n","# ax.set_xticklabels(labels)\n","# ax.legend()\n","\n","# plt.tight_layout()\n","# plt.show()\n","\n","# # Analyze lagged effects\n","# lag_similarities = analyze_lagged_effects(RW_Gamma, AB_Gammas)\n","\n","# # Visualize lagged effect similarities\n","# fig, ax = plt.subplots(figsize=(12, 6))\n","# x = np.arange(len(lag_similarities))\n","# width = 0.25\n","\n","# ax.bar(x - width, lag_similarities[:, 0], width, label='Sign-based', alpha=0.8)\n","# ax.bar(x, lag_similarities[:, 1], width, label='Size-based', alpha=0.8)\n","# ax.bar(x + width, lag_similarities[:, 2], width, label='Conjunction', alpha=0.8)\n","\n","# ax.set_ylabel('Similarity')\n","# ax.set_xlabel('Lag')\n","# ax.set_title('Lagged Effect Similarities')\n","# ax.set_xticks(x)\n","# ax.legend()\n","\n","# plt.tight_layout()\n","# plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":4}
