{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate datasets with different data characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_var_data(B_matrices, T=1000,\n",
    "                      is_linear=True, is_gaussian=True, is_stationary=True,\n",
    "                      nonlinear_strength=0.3, error_scale=0.2, \n",
    "                      trend_probability=0.3, max_trend_strength=0.004,\n",
    "                      fluctuation_scale=0.01, random_state=None):\n",
    "    \"\"\"\n",
    "    Generate VAR data with various characteristics.\n",
    "    \n",
    "    Parameters:\n",
    "    - B_matrices: List of coefficient matrices [B0, B1, B2, ..., Bp] where p is the maximum lag\n",
    "    - T: Number of time points\n",
    "    - is_linear: If False, introduces nonlinear relationships\n",
    "    - is_gaussian: If False, uses non-Gaussian noise\n",
    "    - is_stationary: If False, introduces trends and fluctuations\n",
    "    - nonlinear_strength: Strength of nonlinear relationships\n",
    "    - error_scale: Scale of the error terms\n",
    "    - trend_probability: Probability of a variable having a trend\n",
    "    - max_trend_strength: Strength of the trend (used if not stationary)\n",
    "    - fluctuation_scale: Scale of the fluctuations (used if not stationary)\n",
    "    - random_state: Seed for random number generation\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    T_spurious = 20   # Number of spurious time points to discard\n",
    "    \n",
    "    n_vars = B_matrices[0].shape[0]\n",
    "    n_lags = len(B_matrices) - 1\n",
    "    \n",
    "    # Ensure B0 is lower triangular\n",
    "    B_matrices[0] = np.tril(B_matrices[0], k=-1)\n",
    "    \n",
    "    # Calculate M matrices\n",
    "    M_matrices = []\n",
    "    I_minus_B0_inv = np.linalg.inv(np.eye(n_vars) - B_matrices[0])\n",
    "    for i in range(1, n_lags + 1):\n",
    "        M_matrices.append(np.dot(I_minus_B0_inv, B_matrices[i]))\n",
    "\n",
    "    # Generate error terms (Gaussian or t-distributed)\n",
    "    if is_gaussian:\n",
    "        ee = np.random.normal(size=(n_vars, T + T_spurious))\n",
    "    else:\n",
    "        ee = np.random.standard_t(df=5, size=(n_vars, T + T_spurious))\n",
    "    \n",
    "    ee = ee - np.mean(ee, axis=1, keepdims=True)\n",
    "    ee = ee / np.std(ee, axis=1, keepdims=True)\n",
    "    std_e = (np.random.uniform(size=(n_vars,)) + 0.5) * error_scale\n",
    "    nn = np.dot(I_minus_B0_inv, np.diag(std_e) @ ee)\n",
    "\n",
    "    # Generate time series data\n",
    "    xx = np.zeros((n_vars, T + T_spurious))\n",
    "    base_levels = np.random.uniform(1.0, 5.0, n_vars)\n",
    "    xx[:, :n_lags] = base_levels[:, np.newaxis] + np.random.normal(0, 0.1, (n_vars, n_lags))\n",
    "\n",
    "    for t in range(n_lags, T + T_spurious):\n",
    "        xx[:, t] = sum(np.dot(M, xx[:, t-i]) for i, M in enumerate(M_matrices, start=1))\n",
    "        if not is_linear:\n",
    "            xx[:, t] += nonlinear_strength * np.tanh(xx[:, t-1])\n",
    "        xx[:, t] += nn[:, t]    # Add noise\n",
    "\n",
    "    # Add trends if not stationary\n",
    "    if not is_stationary:\n",
    "        trend_vars = np.random.random(n_vars) < trend_probability\n",
    "        for var in range(n_vars):\n",
    "            if trend_vars[var]:\n",
    "                trend_strength = np.random.uniform(0, max_trend_strength)\n",
    "                trend = np.arange(T + T_spurious) * trend_strength\n",
    "                xx[var, :] += trend\n",
    "\n",
    "    # Add fluctuations to all variables to mimic random walk behavior\n",
    "    for var in range(n_vars):\n",
    "        fluctuation = np.cumsum(np.random.normal(0, fluctuation_scale, T + T_spurious))\n",
    "        xx[var, :] += fluctuation\n",
    "\n",
    "    # Remove the first T_spurious time points\n",
    "    data = xx[:, T_spurious:]\n",
    "    \n",
    "    # Round all elements in data to 4 decimal places\n",
    "    data = np.round(data, decimals=4)\n",
    "\n",
    "    return data, B_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ground_truth(B_matrices, filepath):\n",
    "    with open(filepath, 'w') as f:\n",
    "        for i, B in enumerate(B_matrices):\n",
    "            np.savetxt(f, B, delimiter=',', fmt='%.3f')\n",
    "            if i < len(B_matrices) - 1:\n",
    "                f.write('\\n')\n",
    "\n",
    "def generate_and_save_dataset(config, data_type, dataset_index, output_dir):\n",
    "\n",
    "    data, B_matrices = generate_var_data(**config)\n",
    "    \n",
    "    # Save the data\n",
    "    df = pd.DataFrame(data.T, columns=[f\"x{i}\" for i in range(data.shape[0])])\n",
    "    df.to_csv(f\"{output_dir}/dataset_{dataset_index}.csv\", index=False)\n",
    "    \n",
    "    # Save the ground truth (only for the first dataset of each type)\n",
    "    if dataset_index == 0:\n",
    "        save_ground_truth(B_matrices, f\"{output_dir}/ground_truth.csv\")\n",
    "    \n",
    "    # Optionally, plot the data\n",
    "    plot_time_series(data, f\"{data_type} Dataset {dataset_index}\")\n",
    "    plt.savefig(f\"{output_dir}/plot_{dataset_index}.png\")\n",
    "    plt.close()\n",
    "\n",
    "def generate_B_matrices(n_vars=5, n_lags=1, sparsity=0.7, max_coef=0.5, diag_strength=(0.3, 0.8)):\n",
    "    B_matrices = []\n",
    "    \n",
    "    # B0: Instantaneous effect matrix (lower triangular)\n",
    "    B0 = np.zeros((n_vars, n_vars))\n",
    "    for i in range(1, n_vars):\n",
    "        for j in range(i):\n",
    "            if np.random.rand() > sparsity:\n",
    "                B0[i, j] = np.round(np.random.uniform(-max_coef, max_coef), 3)\n",
    "    B_matrices.append(B0)\n",
    "    \n",
    "    # B1 to Bn: Lagged effect matrices\n",
    "    for lag in range(1, n_lags + 1):\n",
    "        B = np.zeros((n_vars, n_vars))\n",
    "        for i in range(n_vars):\n",
    "            # Diagonal elements (autocorrelation)\n",
    "            if lag == 1:\n",
    "                B[i, i] = np.round(np.random.uniform(diag_strength[0], diag_strength[1]), 3)\n",
    "            \n",
    "            # Off-diagonal elements\n",
    "            for j in range(n_vars):\n",
    "                if i != j and np.random.rand() > sparsity:\n",
    "                    B[i, j] = np.round(np.random.uniform(-max_coef, max_coef), 3)\n",
    "        \n",
    "        B_matrices.append(B)\n",
    "    \n",
    "    B_matrices = stabilize_var_matrices(B_matrices)\n",
    "    return B_matrices\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def stabilize_var_matrices(B_matrices, threshold=0.99, max_iterations=100):\n",
    "    \"\"\"\n",
    "    Stabilize VAR matrices by scaling them until all eigenvalues are below the threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - B_matrices: List of coefficient matrices [B0, B1, B2, ..., Bp]\n",
    "    - threshold: Maximum allowed absolute eigenvalue (default: 0.99)\n",
    "    - max_iterations: Maximum number of scaling iterations (default: 100)\n",
    "\n",
    "    Returns:\n",
    "    - Stabilized B_matrices\n",
    "    \"\"\"\n",
    "    n_vars = B_matrices[0].shape[0]\n",
    "    n_lags = len(B_matrices) - 1\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        # Construct companion matrix\n",
    "        companion = np.zeros((n_vars * n_lags, n_vars * n_lags))\n",
    "        companion[:n_vars, :] = np.hstack(B_matrices[1:])\n",
    "        companion[n_vars:, :-n_vars] = np.eye(n_vars * (n_lags - 1))\n",
    "\n",
    "        # Calculate maximum absolute eigenvalue\n",
    "        max_abs_eigenvalue = np.max(np.abs(np.linalg.eigvals(companion)))\n",
    "\n",
    "        # Check if all eigenvalues are below the threshold\n",
    "        if max_abs_eigenvalue < threshold:\n",
    "            print(f\"Stabilization complete after {iteration + 1} iterations.\")\n",
    "            return B_matrices\n",
    "\n",
    "        # Scale all matrices except B0\n",
    "        scaling_factor = 0.95 / max_abs_eigenvalue\n",
    "        for i in range(1, len(B_matrices)):\n",
    "            B_matrices[i] *= scaling_factor\n",
    "\n",
    "    print(f\"Warning: Maximum iterations reached. Final max eigenvalue: {max_abs_eigenvalue}\")\n",
    "    return B_matrices\n",
    "\n",
    "def plot_time_series(data, title=\"Time Series Plot\"):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i in range(data.shape[0]):\n",
    "        plt.plot(data[i, :], label=f'x{i}')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rename plots\n",
    "# data_type = 'scale_50_var'\n",
    "# output_dir = f'../../data/synthetic/{data_type}'\n",
    "\n",
    "# for i in range(10):\n",
    "#     data = pd.read_csv(f'../../data/synthetic/scale_50_var/dataset_{i}.csv')\n",
    "#     columns = data.columns.tolist()\n",
    "#     if \"Date\" in columns:\n",
    "#         data = data.drop(['Date'], axis=1).values\n",
    "#         columns.remove('Date')\n",
    "#     else:\n",
    "#         data = data.values\n",
    "#     plot_time_series(data.T, f\"scale_20_var Dataset {i}\")\n",
    "#     plt.savefig(f\"{output_dir}/plot_{i}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate multiple datasets for each type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stabilization complete after 2 iterations.\n"
     ]
    }
   ],
   "source": [
    "B_matrices = [\n",
    "    np.array([\n",
    "        [ 0,    0,    0,    0,    0   ],\n",
    "        [ 0.4,  0,    0,    0,    0   ],\n",
    "        [-0.2,  0.3,  0,    0,    0   ],\n",
    "        [ 0,    0.2, -0.1,  0,    0   ],\n",
    "        [ 0.1,  0,    0.3,  0.2,  0   ]\n",
    "    ]),\n",
    "    np.array([\n",
    "        [ 0.4,  0,   -0.2,  0,    0.1],\n",
    "        [ 0,    0.3,  0,    0.15, 0  ],\n",
    "        [-0.1,  0.2,  0.5,  0,    0  ],\n",
    "        [ 0.2,  0,    0,    0.6, -0.1],\n",
    "        [ 0,    0,   -0.15, 0.2,  0.4]\n",
    "    ])\n",
    "]\n",
    "B_matrices = generate_B_matrices(n_vars=20, n_lags=1, sparsity=0.7)\n",
    "\n",
    "config = {\n",
    "    'B_matrices': B_matrices,\n",
    "    'T': 1000,\n",
    "    'is_linear': True,\n",
    "    'is_gaussian': False,\n",
    "    'is_stationary': True,\n",
    "    'trend_probability': 0.3,\n",
    "    'max_trend_strength': 0.005,\n",
    "    'fluctuation_scale': 0.04\n",
    "}\n",
    "\n",
    "data_type = 'scale_50_var'\n",
    "output_dir = f'../../data/synthetic/{data_type}'\n",
    "for i in range(10):  # Generate 10 datasets for each type\n",
    "    generate_and_save_dataset(config, data_type, i, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate multiple datasets for all type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All datasets generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define configurations for different types of data\n",
    "\n",
    "B_matrices_dense = generate_B_matrices(n_vars=5, n_lags=1, sparsity=0.3)\n",
    "B_matrices_sparse = generate_B_matrices(n_vars=5, n_lags=1, sparsity=0.7)\n",
    "B_matrices_15 = generate_B_matrices(n_vars=15, n_lags=1, sparsity=0.7)\n",
    "\n",
    "configs = {\n",
    "    'scale_5_var': {\n",
    "        'B_matrices': B_matrices_sparse,\n",
    "        'T': 1000,\n",
    "        'is_linear': True,\n",
    "        'is_gaussian': False,\n",
    "        'is_stationary': True,\n",
    "        'trend_probability': 0.3,\n",
    "        'max_trend_strength': 0.005,\n",
    "        'fluctuation_scale': 0.04\n",
    "    },\n",
    "    'scale_15_var': {\n",
    "        'B_matrices': B_matrices_15,\n",
    "        'T': 1000,\n",
    "        'is_linear': True,\n",
    "        'is_gaussian': False,\n",
    "        'is_stationary': True,\n",
    "        'trend_probability': 0.3,\n",
    "        'max_trend_strength': 0.005,\n",
    "        'fluctuation_scale': 0.04\n",
    "    },\n",
    "    'linear': {\n",
    "        'B_matrices': B_matrices_sparse,\n",
    "        'T': 1000,\n",
    "        'is_linear': True,\n",
    "        'is_gaussian': False,\n",
    "        'is_stationary': True,\n",
    "        'trend_probability': 0.3,\n",
    "        'max_trend_strength': 0.005,\n",
    "        'fluctuation_scale': 0.04\n",
    "    },\n",
    "    'non_linear': {\n",
    "        'B_matrices': B_matrices_sparse,\n",
    "        'T': 1000,\n",
    "        'is_linear': False,\n",
    "        'is_gaussian': False,\n",
    "        'is_stationary': True,\n",
    "        'trend_probability': 0.3,\n",
    "        'max_trend_strength': 0.005,\n",
    "        'fluctuation_scale': 0.04\n",
    "    },\n",
    "    'gaussian': {\n",
    "        'B_matrices': B_matrices_sparse,\n",
    "        'T': 1000,\n",
    "        'is_linear': True,\n",
    "        'is_gaussian': True,\n",
    "        'is_stationary': True,\n",
    "        'trend_probability': 0.3,\n",
    "        'max_trend_strength': 0.005,\n",
    "        'fluctuation_scale': 0.04\n",
    "    },\n",
    "    'non_gaussian': {\n",
    "        'B_matrices': B_matrices_sparse,\n",
    "        'T': 1000,\n",
    "        'is_linear': True,\n",
    "        'is_gaussian': False,\n",
    "        'is_stationary': True,\n",
    "        'trend_probability': 0.3,\n",
    "        'max_trend_strength': 0.005,\n",
    "        'fluctuation_scale': 0.04\n",
    "    },\n",
    "    'stationary': {\n",
    "        'B_matrices': B_matrices_sparse,\n",
    "        'T': 1000,\n",
    "        'is_linear': True,\n",
    "        'is_gaussian': False,\n",
    "        'is_stationary': True,\n",
    "        'trend_probability': 0.3,\n",
    "        'max_trend_strength': 0.005,\n",
    "        'fluctuation_scale': 0.04\n",
    "    },\n",
    "    'non_stationary': {\n",
    "        'B_matrices': B_matrices_sparse,\n",
    "        'T': 1000,\n",
    "        'is_linear': True,\n",
    "        'is_gaussian': False,\n",
    "        'is_stationary': False,\n",
    "        'trend_probability': 0.3,\n",
    "        'max_trend_strength': 0.005,\n",
    "        'fluctuation_scale': 0.04\n",
    "    },\n",
    "    'sparse': {\n",
    "        'B_matrices': B_matrices_sparse,\n",
    "        'T': 1000,\n",
    "        'is_linear': True,\n",
    "        'is_gaussian': False,\n",
    "        'is_stationary': False,\n",
    "        'trend_probability': 0.3,\n",
    "        'max_trend_strength': 0.005,\n",
    "        'fluctuation_scale': 0.04,\n",
    "    },\n",
    "    'dense': {\n",
    "        'B_matrices': B_matrices_dense,\n",
    "        'T': 1000,\n",
    "        'is_linear': True,\n",
    "        'is_gaussian': False,\n",
    "        'is_stationary': False,\n",
    "        'trend_probability': 0.3,\n",
    "        'max_trend_strength': 0.005,\n",
    "        'fluctuation_scale': 0.04,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Generate datasets for each configuration\n",
    "for config_name, config in configs.items():\n",
    "    output_dir = f'../../data/synthetic/{config_name}'\n",
    "    \n",
    "    for i in range(10):\n",
    "        generate_and_save_dataset(config, config_name, i, output_dir)\n",
    "\n",
    "print(\"All datasets generated successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
